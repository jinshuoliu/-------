# 改善神经网络

## 1. week1:深度学习的实用层面

### 1.1. 训练/开发/测试集

### 1.2. 偏差/方差

### 1.3. 机器学习基础

### 1.4. 正则化

### 1.5. 为什么正则化可以减少过拟合

### 1.6. Dropout正则化

### 1.7. 理解Dropout

### 1.8. 其他正则化方法

### 1.9. 归一化输入

### 1.10. 梯度消失与梯度爆炸

### 1.11. 神经网络的权重初始化

### 1.12. 梯度的数值逼近

### 1.13. 梯度检验

### 1.14. 关于梯度检验实现的注记

### 1.15. 总结

## 2. week2:优化算法

### 2.1. Mini-batch梯度下降

### 2.2. 理解mini-batch梯度下降法

### 2.3. 指数加权平均

### 2.4. 理解指数加权平均

### 2.5. 指数加权平均的偏差修正

### 2.6. 栋梁梯度下降法

### 2.7. RMSprop

### 2.8. Adam优化算法

### 2.9. 学习率衰减

### 2.10. 局部最优的问题

### 2.11. 总结

## 3. week3:超参数调试、Batch正则化和程序框架

### 3.1. 调试处理

### 3.2. 为超参数选择合适的范围

### 3.3. 超参数训练的实践:Pandas vs Caviar

### 3.4. 正则化网络的激活函数

### 3.5. 将Batch Norm拟合进神经网络

### 3.6. Batch Norm为什么奏效

### 3.7. 测试时的Batch Norm

### 3.8. Softmax回归

### 3.9. 训练一个Softmax分类器

### 3.10. 深度学习框架

### 3.11. TensorFlow

### 3.12. 总结

## 4. 采访

### Yoshua

### Yuanqing Lin
